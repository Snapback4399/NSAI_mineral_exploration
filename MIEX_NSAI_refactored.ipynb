{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee9d5c6",
   "metadata": {},
   "source": [
    "# MIEX Cuâ€“Mo Prospectivity (Notebook)\n",
    "\n",
    "This notebook is a **clean, function-based refactor** of the original analysis notebook.\n",
    "It keeps the same workflow, but organizes it into reusable functions and a single\n",
    "`run_pipeline()` entry point.\n",
    "\n",
    "**Main outputs**\n",
    "- Site table with geology + distance features\n",
    "- Knowledge-derived embedding priors (deposit-model similarity)\n",
    "- Spatial-CV anomaly/prospectivity models (baseline vs. knowledge-augmented)\n",
    "- Optional map products (interpolated prospectivity surface, target polygons)\n",
    "\n",
    "> **Note:** Secrets (Azure/OpenAI) and local file paths are **not committed**. Configure via environment variables or the config cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe36d4",
   "metadata": {},
   "source": [
    "## 0) Setup & Configuration\n",
    "Fill in paths and options.  \n",
    "If you run in Colab, you may still mount Drive manually, but the notebook no longer *requires* Colab-specific APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc71410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, json, glob, hashlib, math, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Geo / mapping stack (install if needed)\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# -----------------------\n",
    "# User config (edit me)\n",
    "# -----------------------\n",
    "CONFIG = {\n",
    "    # Inputs\n",
    "    \"csv_points\": \"data/Cumo.csv\",             # <-- path to point CSV\n",
    "    \"geology_root\": \"data/geology/\",           # <-- folder containing geology shapefiles\n",
    "    \"json_dir\": \"data/deposit_models_json/\",   # <-- folder containing 87 deposit-model JSONs\n",
    "\n",
    "    # Column hints (set if auto-detect fails)\n",
    "    \"lon_candidates\": [\"Longitude\", \"lon\", \"LONGITUDE\", \"X\", \"LONG\"],\n",
    "    \"lat_candidates\": [\"Latitude\", \"lat\", \"LATITUDE\", \"Y\", \"LAT\"],\n",
    "\n",
    "    # Spatial settings\n",
    "    \"metric_crs\": \"EPSG:32611\",  # UTM zone used in the original notebook\n",
    "    \"crop_frac\": 0.10,           # keep ~10% window for quick testing; set 1.0 for full run\n",
    "    \"site_grid_m\": 5.0,          # merge duplicate points within this grid (meters)\n",
    "    \"distance_cap_m\": 25_000.0,  # cap distances for stability\n",
    "\n",
    "    # Knowledge priors (embeddings)\n",
    "    \"embedding_provider\": \"azure_openai\",      # \"azure_openai\" | \"openai\"\n",
    "    \"embedding_model\": os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-large\"),\n",
    "    \"embedding_cache\": \"cache/embeddings/\",\n",
    "\n",
    "    # Modeling\n",
    "    \"n_splits\": 5,\n",
    "    \"top_frac_eval\": 0.10,  # recall@topX\n",
    "}\n",
    "os.makedirs(CONFIG[\"embedding_cache\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43fba4",
   "metadata": {},
   "source": [
    "## 1) Utilities (I/O, CRS, helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a468d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_flex(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV with a Latin-1 fallback for messy encodings.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", low_memory=False)\n",
    "\n",
    "def pick_col(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Pick a column name from candidates with tolerant matching.\"\"\"\n",
    "    lower = {c.lower(): c for c in cols}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower:\n",
    "            return lower[c.lower()]\n",
    "    # fuzzy contains\n",
    "    for col in cols:\n",
    "        col_norm = col.lower().replace(\" \", \"\")\n",
    "        for c in candidates:\n",
    "            if c.lower().replace(\" \", \"\") in col_norm:\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def to_metric_points(df: pd.DataFrame, lon_col: str, lat_col: str, metric_crs: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Create GeoDataFrame from lon/lat and project to metric CRS.\"\"\"\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df.copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col].astype(float), df[lat_col].astype(float)),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    return gdf.to_crs(metric_crs)\n",
    "\n",
    "def crop_window(gdf: gpd.GeoDataFrame, frac: float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Keep a center crop window for faster iteration. Use frac=1.0 for full.\"\"\"\n",
    "    if frac is None or frac >= 0.999:\n",
    "        return gdf\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    cx, cy = (xmin+xmax)/2, (ymin+ymax)/2\n",
    "    w, h = (xmax-xmin)*frac, (ymax-ymin)*frac\n",
    "    box = gpd.GeoSeries([Point(cx, cy).buffer(1).envelope], crs=gdf.crs).iloc[0]\n",
    "    # make a rectangle envelope\n",
    "    rect = gpd.GeoSeries([Point(cx, cy).buffer(1).envelope], crs=gdf.crs).iloc[0]\n",
    "    rect = gpd.GeoSeries([Point(cx, cy).buffer(1).envelope], crs=gdf.crs).iloc[0]\n",
    "    # build manual rectangle\n",
    "    from shapely.geometry import box as sbox\n",
    "    rect = sbox(cx - w/2, cy - h/2, cx + w/2, cy + h/2)\n",
    "    return gdf[gdf.geometry.within(rect)].copy()\n",
    "\n",
    "def round_series(x: np.ndarray, step: float) -> np.ndarray:\n",
    "    \"\"\"Round to a metric grid (used to merge near-duplicate samples).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return (np.round(x/step)*step).astype(np.float64)\n",
    "\n",
    "def safe_min_distance(points: np.ndarray, lines_gdf: gpd.GeoDataFrame, cap_m: float) -> np.ndarray:\n",
    "    \"\"\"Compute min distance from each point to a line layer using STRtree. Returns capped meters.\"\"\"\n",
    "    if lines_gdf is None or len(lines_gdf) == 0:\n",
    "        return np.full(len(points), np.nan, dtype=float)\n",
    "\n",
    "    geoms = list(lines_gdf.geometry.values)\n",
    "    tree = STRtree(geoms)\n",
    "\n",
    "    out = np.empty(len(points), dtype=float)\n",
    "    for i, p in enumerate(points):\n",
    "        # query candidates by bbox; STRtree returns geometries\n",
    "        cands = tree.query(p)\n",
    "        if not cands:\n",
    "            out[i] = np.nan\n",
    "            continue\n",
    "        out[i] = float(min(p.distance(g) for g in cands))\n",
    "    if cap_m is not None:\n",
    "        out = np.minimum(out, cap_m)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8fea5",
   "metadata": {},
   "source": [
    "## 2) Load geology layers (polygons + line features)\n",
    "The original notebook searched the geology folder for:\n",
    "- a **polygon** layer (map units)\n",
    "- optional **line** layers (faults / contacts / boundaries)\n",
    "\n",
    "The helper below keeps the same behavior with clear fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_polygon_shp(root: str) -> str:\n",
    "    \"\"\"Find a likely map-unit polygon shapefile under a root folder.\"\"\"\n",
    "    cand = []\n",
    "    for r, _, fs in os.walk(root):\n",
    "        for f in fs:\n",
    "            if f.lower().endswith('.shp'):\n",
    "                fl = f.lower()\n",
    "                fp = os.path.join(r, f)\n",
    "                if 'mapunit' in fl and ('poly' in fl or 'polygon' in fl):\n",
    "                    cand.append(fp)\n",
    "    if cand:\n",
    "        return cand[0]\n",
    "    # fallback: first .shp we can read as polygons\n",
    "    for r, _, fs in os.walk(root):\n",
    "        for f in fs:\n",
    "            if f.lower().endswith('.shp'):\n",
    "                return os.path.join(r, f)\n",
    "    raise FileNotFoundError(f\"No shapefile found under {root}\")\n",
    "\n",
    "def load_geology_layers(geology_root: str, metric_crs: str) -> Tuple[gpd.GeoDataFrame, Dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"Load map-unit polygons and candidate line layers projected to metric CRS.\"\"\"\n",
    "    poly_path = find_polygon_shp(geology_root)\n",
    "    polys = gpd.read_file(poly_path)\n",
    "    polys = polys.to_crs(metric_crs)\n",
    "\n",
    "    # load all line-ish shapefiles as candidates\n",
    "    line_layers: Dict[str, gpd.GeoDataFrame] = {}\n",
    "    for shp in glob.glob(os.path.join(geology_root, '**', '*.shp'), recursive=True):\n",
    "        if shp == poly_path:\n",
    "            continue\n",
    "        try:\n",
    "            g = gpd.read_file(shp)\n",
    "            if g.empty:\n",
    "                continue\n",
    "            geom_type = g.geometry.iloc[0].geom_type.lower()\n",
    "            if 'line' in geom_type:\n",
    "                key = os.path.splitext(os.path.basename(shp))[0]\n",
    "                line_layers[key] = g.to_crs(metric_crs)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return polys, line_layers\n",
    "\n",
    "def pick_unit_key(cols: List[str]) -> Optional[str]:\n",
    "    \"\"\"Pick a unit/mapunit label field from polygon attributes.\"\"\"\n",
    "    L = {c.lower(): c for c in cols}\n",
    "    for k in ['mapunit','unit','unitsym','symbol','label','name','unitcode','unit_id']:\n",
    "        if k in L:\n",
    "            return L[k]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf5485",
   "metadata": {},
   "source": [
    "## 3) Load points, project, and build *sites*\n",
    "The original notebook merges near-duplicate samples into robust \"sites\" using a metric grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points_build_sites(config: dict) -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"Load point CSV -> metric GeoDataFrame -> crop -> aggregate into site table.\"\"\"\n",
    "    df = read_csv_flex(config['csv_points'])\n",
    "    lon_col = pick_col(list(df.columns), config['lon_candidates'])\n",
    "    lat_col = pick_col(list(df.columns), config['lat_candidates'])\n",
    "    if lon_col is None or lat_col is None:\n",
    "        raise RuntimeError(f\"Cannot detect lon/lat columns. Found lon={lon_col}, lat={lat_col}.\")\n",
    "\n",
    "    gpts = to_metric_points(df, lon_col, lat_col, config['metric_crs'])\n",
    "    gpts = crop_window(gpts, config['crop_frac'])\n",
    "\n",
    "    # Create bins for aggregation\n",
    "    G = gpts.copy()\n",
    "    G['x_m'] = G.geometry.x.astype('float64')\n",
    "    G['y_m'] = G.geometry.y.astype('float64')\n",
    "    G['x_bin'] = round_series(G['x_m'].values, config['site_grid_m'])\n",
    "    G['y_bin'] = round_series(G['y_m'].values, config['site_grid_m'])\n",
    "    G['site_id'] = (G['x_bin'].astype(str) + '_' + G['y_bin'].astype(str))\n",
    "\n",
    "    # Aggregate: keep median for numeric, first for non-numeric (same spirit as original)\n",
    "    num_cols = [c for c in G.columns if pd.api.types.is_numeric_dtype(G[c]) and c not in ['x_m','y_m']]\n",
    "    keep_cols = [c for c in G.columns if c not in ['geometry']]\n",
    "    agg = {}\n",
    "    for c in keep_cols:\n",
    "        if c in num_cols:\n",
    "            agg[c] = 'median'\n",
    "        else:\n",
    "            agg[c] = 'first'\n",
    "\n",
    "    U = (G.drop(columns=['geometry'])\n",
    "           .groupby('site_id', dropna=False)\n",
    "           .agg(agg)\n",
    "           .reset_index())\n",
    "\n",
    "    # rebuild geometry from binned coordinates\n",
    "    U_gdf = gpd.GeoDataFrame(\n",
    "        U,\n",
    "        geometry=gpd.points_from_xy(U['x_bin'].astype(float), U['y_bin'].astype(float)),\n",
    "        crs=config['metric_crs']\n",
    "    )\n",
    "    return gpts, U_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb878d",
   "metadata": {},
   "source": [
    "## 4) Spatial joins & distance features\n",
    "- Join each site to a map-unit polygon (`unit_code`)\n",
    "- Compute min distance to each line layer (fault/contact/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc61b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geology_unit(U: gpd.GeoDataFrame, polys: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Spatial join sites to polygons and create a single 'unit_code' column.\"\"\"\n",
    "    unit_col = pick_unit_key(list(polys.columns))\n",
    "    if unit_col is None:\n",
    "        raise RuntimeError('Cannot find a unit field in polygons (mapunit/unit/label/etc.).')\n",
    "\n",
    "    out = gpd.sjoin(U, polys[[unit_col, 'geometry']], how='left', predicate='within')\n",
    "    out = out.rename(columns={unit_col: 'unit_code'}).drop(columns=['index_right'], errors='ignore')\n",
    "    # de-duplicate unit_code if created multiple times\n",
    "    ucols = [c for c in out.columns if c.lower() == 'unit_code']\n",
    "    if len(ucols) > 1:\n",
    "        tmp = out[ucols].copy()\n",
    "        out['unit_code'] = tmp.bfill(axis=1).iloc[:, 0]\n",
    "        out = out.drop(columns=[c for c in ucols if c != 'unit_code'])\n",
    "    return out\n",
    "\n",
    "def add_line_distances(U: gpd.GeoDataFrame, line_layers: Dict[str, gpd.GeoDataFrame], cap_m: float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Add min-distance-to-line features for each line layer.\"\"\"\n",
    "    out = U.copy()\n",
    "    pts = np.array(list(out.geometry.values))\n",
    "    for name, g in line_layers.items():\n",
    "        out[f'dist_to_{name}'] = safe_min_distance(pts, g, cap_m=cap_m)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781353a",
   "metadata": {},
   "source": [
    "## 5) Knowledge priors (deposit-model embeddings + sample similarity)\n",
    "\n",
    "This reproduces the original idea:\n",
    "1) Convert each deposit-model JSON into a stable text\n",
    "2) Embed model texts\n",
    "3) Embed per-sample text (from selected descriptive columns) and compute similarity priors\n",
    "\n",
    "**Important:** This notebook includes a provider-agnostic embedding wrapper.  \n",
    "You must set credentials via environment variables and **never commit secrets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db13443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Embedding client wrappers --------\n",
    "def _hash_text(s: str) -> str:\n",
    "    return hashlib.md5(s.encode('utf-8')).hexdigest()[:12]\n",
    "\n",
    "def build_model_text(j: dict) -> str:\n",
    "    \"\"\"Convert a deposit-model JSON dict into a compact, stable text for embedding.\"\"\"\n",
    "    keep_keys = [\n",
    "        'Model_Index','Model_Name','APPROXIMATE_SYNONYM','DESCRIPTION',\n",
    "        'Rock_Types','Depositional_Environment','Tectonic_Settings',\n",
    "        'Alteration','Mineralogy','Ore_Controls','Geochemical_Signature',\n",
    "        'Associated_Deposit_Types','Weathering','Texture_Structure','Textures',\n",
    "        'Age_Range','GEOLOGIC_AGE','HOST_ROCK_TYPE','MINERALIZATION'\n",
    "    ]\n",
    "    lines = []\n",
    "    for k in keep_keys:\n",
    "        if k in j and j[k] not in [None, '', [], {}]:\n",
    "            v = j[k]\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                v = '; '.join([str(x) for x in v if x is not None])\n",
    "            elif isinstance(v, dict):\n",
    "                v = json.dumps(v, ensure_ascii=False)\n",
    "            lines.append(f\"{k}: {v}\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def load_deposit_models(json_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Load deposit-model JSON files and return a table with text for embedding.\"\"\"\n",
    "    paths = sorted(glob.glob(os.path.join(json_dir, '*.json')))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {json_dir}\")\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            j = json.load(f)\n",
    "        rows.append({\n",
    "            'path': p,\n",
    "            'model_id': os.path.splitext(os.path.basename(p))[0],\n",
    "            'model_text': build_model_text(j),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_embedding_client(config: dict):\n",
    "    \"\"\"Return a callable(texts)->np.ndarray. Supports Azure OpenAI or OpenAI.\"\"\"\n",
    "    provider = config.get('embedding_provider', 'azure_openai').lower()\n",
    "\n",
    "    if provider == 'azure_openai':\n",
    "        # Expected env vars:\n",
    "        # AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_VERSION (optional)\n",
    "        from openai import AzureOpenAI\n",
    "        api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
    "        deployment = os.getenv('AZURE_OPENAI_EMBED_DEPLOYMENT', config['embedding_model'])\n",
    "        if not api_key or not endpoint:\n",
    "            raise RuntimeError('Missing AZURE_OPENAI_API_KEY / AZURE_OPENAI_ENDPOINT.')\n",
    "        client = AzureOpenAI(api_key=api_key, azure_endpoint=endpoint, api_version=api_version)\n",
    "\n",
    "        def embed(texts: List[str]) -> np.ndarray:\n",
    "            out = []\n",
    "            for t in texts:\n",
    "                resp = client.embeddings.create(model=deployment, input=t)\n",
    "                out.append(resp.data[0].embedding)\n",
    "            return np.asarray(out, dtype=float)\n",
    "        return embed\n",
    "\n",
    "    elif provider == 'openai':\n",
    "        from openai import OpenAI\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise RuntimeError('Missing OPENAI_API_KEY.')\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        model = config['embedding_model']\n",
    "\n",
    "        def embed(texts: List[str]) -> np.ndarray:\n",
    "            out = []\n",
    "            for t in texts:\n",
    "                resp = client.embeddings.create(model=model, input=t)\n",
    "                out.append(resp.data[0].embedding)\n",
    "            return np.asarray(out, dtype=float)\n",
    "        return embed\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown embedding_provider: {provider}\")\n",
    "\n",
    "def embed_with_cache(texts: List[str], embed_fn, cache_dir: str, prefix: str) -> np.ndarray:\n",
    "    \"\"\"Embed texts with per-text caching to avoid re-calling the API.\"\"\"\n",
    "    embs = []\n",
    "    for t in texts:\n",
    "        hid = _hash_text(t)\n",
    "        fp = os.path.join(cache_dir, f\"{prefix}_{hid}.npy\")\n",
    "        if os.path.exists(fp):\n",
    "            embs.append(np.load(fp))\n",
    "        else:\n",
    "            v = embed_fn([t])[0]\n",
    "            np.save(fp, np.asarray(v, dtype=float))\n",
    "            embs.append(v)\n",
    "    return np.asarray(embs, dtype=float)\n",
    "\n",
    "def build_sample_text(row: pd.Series, text_cols: List[str]) -> str:\n",
    "    \"\"\"Build a per-sample text from selected descriptive columns.\"\"\"\n",
    "    parts = []\n",
    "    for c in text_cols:\n",
    "        if c in row.index and pd.notnull(row[c]) and str(row[c]).strip() != '':\n",
    "            parts.append(f\"{c}: {str(row[c]).strip()}\")\n",
    "    return '\\n'.join(parts)\n",
    "\n",
    "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Cosine similarity between rows of A and rows of B.\"\"\"\n",
    "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
    "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
    "    return A @ B.T\n",
    "\n",
    "# Text columns used in the original notebook's 'deployable' set (safe defaults; adjust as needed)\n",
    "TEXT_COLS_DEPLOYABLE = [\n",
    "    \"SAMPLE_SOURCE\",\"METHOD_COLLECTED\",\"GEOLOGIC_AGE\",\"STRATIGRAPHY\",\n",
    "    \"HOST_NAME\",\"HOST_ROCK_TYPE\",\"MINERALIZATION\",\"ALTERATION\",\n",
    "    \"IGNEOUS_FORM\",\"METAMORPHISM\",\"FACIES_GRADE\",\"LOCATE_DESC\",\n",
    "    \"COORDINATES_COMMENT\",\"SAMPLE_COMMENT_ST\",\"SAMPLE_COMMENT_LT\",\"ADDL_ATTR\"\n",
    "]\n",
    "\n",
    "def add_knowledge_priors(U: gpd.GeoDataFrame, config: dict) -> Tuple[gpd.GeoDataFrame, pd.DataFrame]:\n",
    "    \"\"\"Compute embedding-based similarity priors and add them as columns to U.\"\"\"\n",
    "    models = load_deposit_models(config['json_dir'])\n",
    "    embed_fn = get_embedding_client(config)\n",
    "\n",
    "    model_emb = embed_with_cache(models['model_text'].tolist(), embed_fn, config['embedding_cache'], prefix='model')\n",
    "    models['emb_dim'] = model_emb.shape[1]\n",
    "\n",
    "    # Sample text -> embeddings\n",
    "    text_cols = [c for c in TEXT_COLS_DEPLOYABLE if c in U.columns]\n",
    "    if not text_cols:\n",
    "        raise RuntimeError('No text columns found in U for sample embedding. Adjust TEXT_COLS_DEPLOYABLE.')\n",
    "    sample_texts = [build_sample_text(U.iloc[i], text_cols) for i in range(len(U))]\n",
    "    sample_emb = embed_with_cache(sample_texts, embed_fn, config['embedding_cache'], prefix='sample')\n",
    "\n",
    "    # Similarity priors\n",
    "    sim = cosine_sim_matrix(sample_emb, model_emb)  # (n_samples, n_models)\n",
    "    # Common, stable priors:\n",
    "    prior_max = sim.max(axis=1)\n",
    "    prior_mean_top10 = np.sort(sim, axis=1)[:, -min(10, sim.shape[1]):].mean(axis=1)\n",
    "\n",
    "    out = U.copy()\n",
    "    out['prior_sim_max'] = prior_max\n",
    "    out['prior_sim_top10_mean'] = prior_mean_top10\n",
    "\n",
    "    # Optional: PCA of model similarities (fold-safe PCA happens in modeling)\n",
    "    # Keep raw similarities only if you really need them (they can be huge)\n",
    "    return out, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee664ff1",
   "metadata": {},
   "source": [
    "## 6) Modeling (spatial CV) + metrics\n",
    "\n",
    "This block provides:\n",
    "- baseline model (geochem + spatial features)\n",
    "- knowledge-augmented model (+ priors)\n",
    "\n",
    "You can plug in your own target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add262f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_top_area(y_true: np.ndarray, y_score: np.ndarray, top_frac: float = 0.10) -> float:\n",
    "    \"\"\"Recall among top fraction of highest-scoring samples (proxy for targeting efficiency).\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "    n = len(y_true)\n",
    "    k = max(1, int(np.ceil(n * top_frac)))\n",
    "    idx = np.argsort(-y_score)[:k]\n",
    "    return float(y_true[idx].sum() / max(1, y_true.sum()))\n",
    "\n",
    "def build_pipeline(feature_cols: List[str], use_pca: bool = False, pca_dim: int = 16) -> Pipeline:\n",
    "    \"\"\"Build a robust preprocessing + classifier pipeline.\"\"\"\n",
    "    transformers = []\n",
    "    transformers.append((\n",
    "        'num',\n",
    "        Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler(with_centering=True)),\n",
    "            *([('pca', PCA(n_components=pca_dim, random_state=0))] if use_pca else [])\n",
    "        ]),\n",
    "        feature_cols\n",
    "    ))\n",
    "\n",
    "    pre = ColumnTransformer(transformers=transformers, remainder='drop', verbose_feature_names_out=False)\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=600,\n",
    "        random_state=0\n",
    "    )\n",
    "    return Pipeline([('pre', pre), ('clf', clf)])\n",
    "\n",
    "def spatial_cv_scores(df: pd.DataFrame, y_col: str, group_col: str, feature_cols: List[str], n_splits: int, top_frac: float) -> Dict[str, float]:\n",
    "    \"\"\"GroupKFold CV evaluation returning AUC/PR-AUC/recall@top.\"\"\"\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[y_col].astype(int).values\n",
    "    groups = df[group_col].values\n",
    "\n",
    "    cv = GroupKFold(n_splits=n_splits)\n",
    "    oof = np.zeros(len(df), dtype=float)\n",
    "\n",
    "    pipe = build_pipeline(feature_cols, use_pca=False)\n",
    "    for tr, te in cv.split(X, y, groups):\n",
    "        pipe.fit(X.iloc[tr], y[tr])\n",
    "        oof[te] = pipe.predict_proba(X.iloc[te])[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y, oof) if len(np.unique(y)) > 1 else np.nan\n",
    "    ap = average_precision_score(y, oof) if len(np.unique(y)) > 1 else np.nan\n",
    "    rtop = recall_at_top_area(y, oof, top_frac=top_frac)\n",
    "    return {'auc': float(auc), 'ap': float(ap), f'recall_top_{int(top_frac*100)}pct': float(rtop)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da3d00",
   "metadata": {},
   "source": [
    "## 7) Pipeline runner\n",
    "\n",
    "This is the only cell you should need to run end-to-end (after setting CONFIG).\n",
    "You still control:\n",
    "- how you define labels (Cu/Mo thresholds, joint anomaly, etc.)\n",
    "- which feature columns are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63296be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(config: dict) -> Dict[str, object]:\n",
    "    \"\"\"Run the core workflow: load -> sites -> geology features -> (optional) knowledge priors.\"\"\"\n",
    "    polys, line_layers = load_geology_layers(config['geology_root'], config['metric_crs'])\n",
    "    gpts, U = load_points_build_sites(config)\n",
    "\n",
    "    U = add_geology_unit(U, polys)\n",
    "    U = add_line_distances(U, line_layers, cap_m=config['distance_cap_m'])\n",
    "\n",
    "    results = {'points': gpts, 'sites': U, 'polys': polys, 'line_layers': line_layers}\n",
    "    return results\n",
    "\n",
    "# Example:\n",
    "# out = run_pipeline(CONFIG)\n",
    "# U = out['sites']\n",
    "# U.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54870eb",
   "metadata": {},
   "source": [
    "## 8) Example: define labels + run baseline vs NSAI\n",
    "\n",
    "Below is a minimal template. Replace the label logic with your study definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519dee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example label definition (placeholder) ---\n",
    "# You MUST adjust this to match your manuscript definitions.\n",
    "def define_example_labels(U: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Example only: define a binary label from Cu and/or Mo columns if present.\"\"\"\n",
    "    out = U.copy()\n",
    "    # Try common Cu/Mo names\n",
    "    cu_col = next((c for c in out.columns if c.lower() in ['cu_ppm','cu','copper_ppm','cu_ppb']), None)\n",
    "    mo_col = next((c for c in out.columns if c.lower() in ['mo_ppm','mo','molybdenum_ppm','mo_ppb']), None)\n",
    "\n",
    "    if cu_col is None and mo_col is None:\n",
    "        raise RuntimeError('Could not find Cu/Mo columns. Please edit label definition.')\n",
    "\n",
    "    # Simple threshold as an example (not your real science)\n",
    "    if cu_col is not None:\n",
    "        cu = pd.to_numeric(out[cu_col], errors='coerce')\n",
    "    else:\n",
    "        cu = pd.Series(np.nan, index=out.index)\n",
    "\n",
    "    if mo_col is not None:\n",
    "        mo = pd.to_numeric(out[mo_col], errors='coerce')\n",
    "    else:\n",
    "        mo = pd.Series(np.nan, index=out.index)\n",
    "\n",
    "    out['y_anom'] = ((cu > np.nanpercentile(cu, 95)) | (mo > np.nanpercentile(mo, 95))).astype(int)\n",
    "    return out\n",
    "\n",
    "# --- Example feature sets ---\n",
    "def guess_baseline_features(U: gpd.GeoDataFrame) -> List[str]:\n",
    "    \"\"\"Heuristic feature selection: geochem numeric columns + distances.\"\"\"\n",
    "    dist_cols = [c for c in U.columns if c.startswith('dist_to_')]\n",
    "    # include common geochem columns if present\n",
    "    numeric_cols = [c for c in U.columns if pd.api.types.is_numeric_dtype(U[c])]\n",
    "    # remove target + identifiers\n",
    "    drop = set(['y_anom'])\n",
    "    feats = [c for c in numeric_cols if c not in drop]\n",
    "    # prioritize dist features + numeric; user can edit\n",
    "    return sorted(set(dist_cols + feats))\n",
    "\n",
    "# --- Run template ---\n",
    "# out = run_pipeline(CONFIG)\n",
    "# U = out['sites']\n",
    "# U = define_example_labels(U)\n",
    "\n",
    "# (Optional) add priors\n",
    "# U, models = add_knowledge_priors(U, CONFIG)\n",
    "\n",
    "# group column for spatial CV: you should define bins / blocks; here's a simple example using x_bin/y_bin coarsening\n",
    "# U['cv_block'] = (round_series(U['x_bin'].values, 500.0).astype(int).astype(str) + '_' +\n",
    "#                  round_series(U['y_bin'].values, 500.0).astype(int).astype(str))\n",
    "\n",
    "# feats_base = guess_baseline_features(U)\n",
    "# scores_base = spatial_cv_scores(U, y_col='y_anom', group_col='cv_block', feature_cols=feats_base, n_splits=CONFIG['n_splits'], top_frac=CONFIG['top_frac_eval'])\n",
    "# print('Baseline:', scores_base)\n",
    "\n",
    "# feats_nsai = feats_base + ['prior_sim_max','prior_sim_top10_mean']\n",
    "# scores_nsai = spatial_cv_scores(U, y_col='y_anom', group_col='cv_block', feature_cols=feats_nsai, n_splits=CONFIG['n_splits'], top_frac=CONFIG['top_frac_eval'])\n",
    "# print('NSAI:', scores_nsai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab22d60",
   "metadata": {},
   "source": [
    "## Notes for GitHub\n",
    "\n",
    "- Put large data (CSV, shapefiles, JSON models) in `data/` **but add to `.gitignore`**.\n",
    "- Put API keys in environment variables (never in notebook output).\n",
    "- Commit the refactored notebook + README."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
